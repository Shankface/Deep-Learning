{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcJ1mJmxiYlh",
        "outputId": "3a9d284b-dd0f-4095-9741-cfd75d27a5b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "----- MODEL FOR CIFAR10 -----\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                131136    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 689,930\n",
            "Trainable params: 688,522\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n",
            "None\n",
            "---------------------------------------------------\n",
            "Top-1 Accuracy: 0.8745\n",
            "Top-5 Accuracy: 0.9958\n",
            "\n",
            "\n",
            "\n",
            "----- MODEL FOR CIFAR100 -----\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 833,572\n",
            "Trainable params: 832,036\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            "None\n",
            "---------------------------------------------------\n",
            "Top-1 Accuracy: 0.6289\n",
            "Top-5 Accuracy: 0.8803\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import datasets, layers, models\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#####\n",
        "# Ayden Shankman\n",
        "# ECE-472\n",
        "# Assigment 4\n",
        "#####\n",
        "# Got help from https://www.tensorflow.org/tutorials/images/cnn\\\n",
        "\n",
        "\n",
        "# Get top-1 and top-k accuracy\n",
        "def top_k_accuracy(k, x_test, y_test, model):\n",
        "    top_1_correct = 0\n",
        "    top_k_correct = 0\n",
        "    p = model.predict(x_test)\n",
        "    for i, prob in enumerate(p):\n",
        "        if(y_test[i][0] in np.argsort(prob)[-1:]):\n",
        "            top_1_correct += 1\n",
        "        if(y_test[i][0] in np.argsort(prob)[-k:]):\n",
        "            top_k_correct += 1\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print(\"Top-1 Accuracy: \" +  str(top_1_correct/len(x_test)))\n",
        "    print(\"Top-\" + str(k) + \" Accuracy: \" +  str(top_k_correct/len(x_test)))\n",
        "\n",
        "# Formatting and Normalizing Data\n",
        "def format_data(dataset):\n",
        "    train_img, Y_train, test_img, y_test = [[0],[0],[0],[0]]\n",
        "    if(dataset == 'cifar10'):\n",
        "        (train_img, Y_train), (test_img, y_test) = datasets.cifar10.load_data()\n",
        "    if(dataset == 'cifar100'):\n",
        "        (train_img, Y_train), (test_img, y_test) = datasets.cifar100.load_data()\n",
        "\n",
        "    # Normalize pixel values between 0-1 instead of 0-255\n",
        "    X_train, x_test = train_img/255.0, test_img/255.0\n",
        "\n",
        "    # Split training into training and validation\n",
        "    x_train = X_train[0:int(len(X_train)-len(X_train)/5)]\n",
        "    x_val = X_train[int(len(X_train)-len(X_train)/5):len(X_train)]\n",
        "    y_train = Y_train[0:int(len(Y_train)-len(Y_train)/5)]\n",
        "    y_val = Y_train[int(len(Y_train)-len(Y_train)/5):len(Y_train)]\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "\n",
        "# Building and Training Model\n",
        "def train_model(x_train, y_train, x_val, y_val, type, name, epochs):\n",
        "    print(y_train)\n",
        "    # Data augmentation. Help from https://stepup.ai/train_data_augmentation_keras/\n",
        "    datagen = ImageDataGenerator(\n",
        "            featurewise_center=False, \n",
        "            samplewise_center=False,\n",
        "            featurewise_std_normalization=False,\n",
        "            samplewise_std_normalization=False,\n",
        "            zca_whitening=False,\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images horizontally\n",
        "            vertical_flip=False)\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(layers.Conv2D(64, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu', input_shape = (32,32,3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.Conv2D(64, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(2,2))\n",
        "    model.add(layers.Dropout(0.05))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(2,2))\n",
        "    model.add(layers.Dropout(0.1))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (3,3), padding = \"same\", kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(2,2))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, kernel_initializer='he_uniform', activation = 'relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    if(type == 'cifar10'):        \n",
        "        model.add(layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "    elif(type == 'cifar100'):\n",
        "        model.add(layers.Dense(100, activation = 'softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    adam = Adam(learning_rate = 0.001)\n",
        "\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  optimizer= adam,\n",
        "                  metrics=['accuracy','sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "    history = model.fit(datagen.flow(x_train, y_train, batch_size=64), \n",
        "                        steps_per_epoch = x_train.shape[0] // 64, \n",
        "                        validation_data=(x_val, y_val),                \n",
        "                        epochs=epochs, \n",
        "                        verbose = 1)\n",
        "\n",
        "    model.save(name)\n",
        "    return model, history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    x_train_cifar10, y_train_cifar10, x_val_cifar10, y_val_cifar10, x_test_cifar10, y_test_cifar10 = format_data('cifar10')\n",
        "    x_train_cifar100, y_train_cifar100, x_val_cifar100, y_val_cifar100, x_test_cifar100, y_test_cifar100 = format_data('cifar100')\n",
        "\n",
        "    train = False\n",
        "    if(train):\n",
        "        model,history = train_model(x_train_cifar100, y_train_cifar100, x_val_cifar100, y_val_cifar100, 'cifar100', name = 'model_cifar100_2.h5', epochs = 50)\n",
        "        top_k_accuracy(5, x_test_cifar100, y_test_cifar100, model)\n",
        "    else:\n",
        "        # Cifar10 dataset\n",
        "        print(\"\\n\\n\\n----- MODEL FOR CIFAR10 -----\")\n",
        "        model_cifar10 = keras.models.load_model('model_cifar10.h5')\n",
        "        print(model_cifar10.summary())\n",
        "        top_k_accuracy(5, x_test_cifar10, y_test_cifar10, model_cifar10)\n",
        "        \n",
        "        # Cifar100 dataset\n",
        "        print(\"\\n\\n\\n----- MODEL FOR CIFAR100 -----\")\n",
        "        model_cifar100 = keras.models.load_model('model_cifar100.h5')\n",
        "        print(model_cifar100.summary())\n",
        "        top_k_accuracy(5, x_test_cifar100, y_test_cifar100, model_cifar100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First model I tried was first 2 convolutionary layers with filter size 32 then a maxpooling layer. I then repeated this again but with filter size 64 then with 128 followed by a dense layer of 128.\n",
        "\n",
        "Next iteration I added Batch normalization after each double layer of convolution right before the max pooling layer and also after the 128 dense layer. This gave me higher accuracy.\n",
        "\n",
        "Next iteration I changed the padding on the convolutions from 'valid' to 'same' and made the kernal initializer 'he_uniform'. This provided a slightly higher accuracy.\n",
        "\n",
        "Next iteration I increased the dropout rate after each layer, to negate some of the over-fitting, and added an extra level of batch normalization in between each double convolution. Once again this raised the accuracy.\n",
        "\n",
        "Finally, I changed the patter of convolution from 32, 64, 128 to 64, 64, 128. After training this model for 50 epochs with a learning rate of .001, I was able to achieve over 87% accuracy for the cifar10 dataset and a 88% top-5 accuracy for the cifar100 dataset. These were the highest accuracies I have achieved."
      ],
      "metadata": {
        "id": "Z1mdM5xEKKyg"
      }
    }
  ]
}